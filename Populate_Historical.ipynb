{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf5dd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SETTLEMENTDATE</th>\n",
       "      <th>REGIONID</th>\n",
       "      <th>TOTALDEMAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:30:00.000</td>\n",
       "      <td>NSW1</td>\n",
       "      <td>7259.07000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:30:00.000</td>\n",
       "      <td>QLD1</td>\n",
       "      <td>6462.14000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:30:00.000</td>\n",
       "      <td>SA1</td>\n",
       "      <td>1395.21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:30:00.000</td>\n",
       "      <td>TAS1</td>\n",
       "      <td>863.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:30:00.000</td>\n",
       "      <td>VIC1</td>\n",
       "      <td>4097.58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481675</th>\n",
       "      <td>2022-07-01 00:00:00.000</td>\n",
       "      <td>NSW1</td>\n",
       "      <td>8731.47500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481676</th>\n",
       "      <td>2022-07-01 00:00:00.000</td>\n",
       "      <td>QLD1</td>\n",
       "      <td>5624.54500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481677</th>\n",
       "      <td>2022-07-01 00:00:00.000</td>\n",
       "      <td>SA1</td>\n",
       "      <td>1694.35000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481678</th>\n",
       "      <td>2022-07-01 00:00:00.000</td>\n",
       "      <td>TAS1</td>\n",
       "      <td>1134.75833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481679</th>\n",
       "      <td>2022-07-01 00:00:00.000</td>\n",
       "      <td>VIC1</td>\n",
       "      <td>5529.65667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481680 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 SETTLEMENTDATE REGIONID  TOTALDEMAND\n",
       "0       2017-01-01 00:30:00.000     NSW1   7259.07000\n",
       "1       2017-01-01 00:30:00.000     QLD1   6462.14000\n",
       "2       2017-01-01 00:30:00.000      SA1   1395.21000\n",
       "3       2017-01-01 00:30:00.000     TAS1    863.08000\n",
       "4       2017-01-01 00:30:00.000     VIC1   4097.58000\n",
       "...                         ...      ...          ...\n",
       "481675  2022-07-01 00:00:00.000     NSW1   8731.47500\n",
       "481676  2022-07-01 00:00:00.000     QLD1   5624.54500\n",
       "481677  2022-07-01 00:00:00.000      SA1   1694.35000\n",
       "481678  2022-07-01 00:00:00.000     TAS1   1134.75833\n",
       "481679  2022-07-01 00:00:00.000     VIC1   5529.65667\n",
       "\n",
       "[481680 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "xlsx_file = \"Empower.csv\"\n",
    "init_data = pd.read_csv(xlsx_file)\n",
    "init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35744ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d945796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "481675    NaN\n",
      "481676    NaN\n",
      "481677    NaN\n",
      "481678    NaN\n",
      "481679    NaN\n",
      "Name: POWER_DAILY, Length: 481680, dtype: float64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "xlsx_file = \"ElecMMS3_ROOFTOP_PV_ACTUAL_pivot.csv\"\n",
    "PV_pivot_data = pd.read_csv(xlsx_file)\n",
    "# PV_pivot_data = PV_pivot_data.fillna(0)\n",
    "print(str(PV_pivot_data['POWER_DAILY']))\n",
    "print(type(init_data['SETTLEMENTDATE']))\n",
    "\n",
    "init_data['added_PV_TOTALDEMAND'] = init_data['TOTALDEMAND']\n",
    "for i in range(len(init_data)):\n",
    "    if not pd.isna(PV_pivot_data['POWER_DAILY'][i]): \n",
    "        init_data['added_PV_TOTALDEMAND'][i] = PV_pivot_data['POWER_DAILY'][i] + init_data['TOTALDEMAND'][i]\n",
    "    else:\n",
    "        if pd.isna(PV_pivot_data['POWER_DAILY'][i]) and not pd.isna(PV_pivot_data['POWER_MEASUREMENT'][i]):\n",
    "            init_data['added_PV_TOTALDEMAND'][i] = PV_pivot_data['POWER_MEASUREMENT'][i] + init_data['TOTALDEMAND'][i]\n",
    "PV_pivot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = \"MarketData_dbo_Public_Holidays.csv\"\n",
    "Public_Holidays_data = pd.read_csv(xlsx_file)\n",
    "Public_Holidays_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472640a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Public_Holidays_data['Timedate'] = pd.to_datetime(Public_Holidays_data['Date'], format=\"%Y-%m-%d\")\n",
    "Public_Holidays_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56768856",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSW_data = init_data[init_data.REGIONID == 'NSW1'].reset_index()\n",
    "QLD_data = init_data[init_data.REGIONID == 'QLD1'].reset_index()\n",
    "SA_data = init_data[init_data.REGIONID == 'SA1'].reset_index()\n",
    "TAS_data = init_data[init_data.REGIONID == 'TAS1'].reset_index()\n",
    "VIC_data = init_data[init_data.REGIONID == 'VIC1'].reset_index()\n",
    "df = NSW_data\n",
    "NSW_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ab8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSW_data['date'] = pd.to_datetime(df['SETTLEMENTDATE'], format=\"%Y-%m-%d %H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af784b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Public_Holidays_data['date'] = pd.to_datetime(Public_Holidays_data['Date'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "Public_Holidays_data\n",
    "df['is_public_holiday'] = 0\n",
    "for j in range(len(df)):\n",
    "    for i in range(len(Public_Holidays_data)):\n",
    "        if Public_Holidays_data['Official_NSW'][i] == True or Public_Holidays_data['AFMA_NSW'][i] == True or Public_Holidays_data['ASX_NSW'][i] == True:\n",
    "            if Public_Holidays_data['Timedate'][i] == NSW_data['date'][j].date():\n",
    "                df['is_public_holiday'][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(NSW_data.added_PV_TOTALDEMAND.copy())\n",
    "data.columns = [\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fe073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.index = data.index.to_datetime()\n",
    "data[\"hour\"] = df.date.dt.hour\n",
    "data[\"weekday\"] = df.date.dt.weekday\n",
    "data[\"month\"] = df.date.dt.month\n",
    "data[\"year\"] = df.date.dt.year\n",
    "data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "data[\"is_public_holiday\"] = df.is_public_holiday\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9916e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = \"-33.86882_151.209295_Solcast_PT30M.csv\"   #NSW\n",
    "weather_data = pd.read_csv(xlsx_file)\n",
    "weather_data['PeriodStart'][0]\n",
    "weather_data['date'] = pd.to_datetime(weather_data['PeriodEnd'], format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "weather_data\n",
    "from datetime import timedelta\n",
    "weather_data['date'] = weather_data['date'] + timedelta(hours=10)\n",
    "weather_data\n",
    "new_weather = weather_data.loc[weather_data['date']>='2017-01-01 01:00:00']\n",
    "new_weather = new_weather.loc[new_weather['date']<='2022-07-01 00:00:00']\n",
    "new_weather\n",
    "new = pd.concat([new_weather,df],axis=0)\n",
    "new_merge = df.merge(new_weather,on=['date'])\n",
    "# # new_merge = new_merge.rename(columns={'TOTALDEMAND':'y'})\n",
    "new_merge = new_merge.reset_index()\n",
    "df_col = pd.concat([new_merge,data], axis=1)\n",
    "train_data = df_col.drop(columns=['level_0','index','SETTLEMENTDATE','REGIONID','TOTALDEMAND','added_PV_TOTALDEMAND','date','PeriodEnd','PeriodStart','Period'])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = train_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lognorm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from xgboost import XGBRegressor \n",
    "from xgboost_distribution import XGBDistribution\n",
    "\n",
    "def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False, scale=19.6):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(600, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "#     plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "\n",
    "    print(y_test)\n",
    "    print(prediction)\n",
    "    plt.show()\n",
    "\n",
    "xgb = XGBDistribution(distribution=\"log-normal\")\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "testpred = xgb.predict(X_test_scaled)\n",
    "\n",
    "mean, std = testpred.scale, testpred.s\n",
    "\n",
    "\n",
    "plt.figure(figsize=(600, 7))\n",
    "plt.plot(mean, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "plt.plot(mean+np.exp(std), \"r--\",label=\"upper\", alpha=0.5,linewidth=2.0)\n",
    "plt.plot(mean-np.exp(std),\"r--\", label=\"lower\", alpha=0.5,linewidth=2.0)\n",
    "error = mean_absolute_percentage_error(mean, y_test)\n",
    "# print(y_test)\n",
    "# print(prediction)\n",
    "print(error)\n",
    "plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "rmse = np.sqrt(mean_squared_error(mean, y_test))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True);\n",
    "plt.show()\n",
    "\n",
    "print(std)\n",
    "\n",
    "\n",
    "print(testpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1094dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "\n",
    "fig = plt.figure(figsize =(180, 7))\n",
    "ax = fig.add_axes([1, 1, 1, 1])\n",
    "data = []\n",
    "for i in range(200): #change to any numbers\n",
    "    np.random.seed(1)\n",
    "\n",
    "#generate log-normal distributed random variable with 1000 values\n",
    "    lognorm_values = lognorm.rvs(s=testpred.s[i], scale=testpred.scale[i], size=1000)\n",
    "    data.append(lognorm_values)\n",
    "ax.set_xticklabels(X_test.hour[0:200])    \n",
    "ax.boxplot(x=data)\n",
    "# plt.plot(y_test_reset[0:200])\n",
    "# ax.plot(y_test_reset[0:200],linewidth = 2.0)\n",
    "# ax.plot(y_test[0:200])\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b57f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(X_train.columns, xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d761ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can change to specific number\n",
    "AirTemp_weight = xgb.feature_importances_[1]\n",
    "Azimuth_weight = xgb.feature_importances_[2]\n",
    "CloudOpacity_weight = xgb.feature_importances_[3]\n",
    "DewpointTemp_weight = xgb.feature_importances_[4]\n",
    "Dhi_weight = xgb.feature_importances_[5]\n",
    "Dni_weight = xgb.feature_importances_[6]\n",
    "Ebh_weight = xgb.feature_importances_[7]\n",
    "Ghi_weight = xgb.feature_importances_[8]\n",
    "PrecipitableWater_weight = xgb.feature_importances_[9]\n",
    "RelativeHumidity_weight = xgb.feature_importances_[10]\n",
    "SnowWater_weight = xgb.feature_importances_[11]\n",
    "SurfacePressure_weight = xgb.feature_importances_[12]\n",
    "WindDirection10m_weight = xgb.feature_importances_[13]\n",
    "WindSpeed10m_weight = xgb.feature_importances_[14]\n",
    "Zenith_weight = xgb.feature_importances_[15]\n",
    "AlbedoDaily_weight = xgb.feature_importances_[16]\n",
    "hour_weight = xgb.feature_importances_[17]\n",
    "weekday_weight = xgb.feature_importances_[18]\n",
    "month_weight = xgb.feature_importances_[19]\n",
    "year_weight = xgb.feature_importances_[20]\n",
    "is_weekend_weight = xgb.feature_importances_[21]\n",
    "is_public_holiday_weight = xgb.feature_importances_[22] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirTemp_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SurfacePressure'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_diff = 9999\n",
    "for index,row in df_col.iterrows():\n",
    "    AirTemp = (row.AirTemp - float(test.AirTemp))**2 * AirTemp_weight\n",
    "    Azimuth = (row.Azimuth - float(test.Azimuth))**2 * Azimuth_weight\n",
    "    CloudOpacity = (row.CloudOpacity - float(test.CloudOpacity))**2 * CloudOpacity_weight\n",
    "    DewpointTemp = (row.DewpointTemp - float(test.DewpointTemp))**2 * DewpointTemp_weight\n",
    "    Dhi = (row.Dhi - float(test.Dhi))**2 * Dhi_weight\n",
    "    Dni = (row.Dni - float(test.Dni))**2 * Dni_weight\n",
    "    Ebh = (row.Ebh - float(test.Ebh))**2 * Ebh_weight\n",
    "    Ghi = (row.Ghi - float(test.Ghi))**2 * Ghi_weight\n",
    "    PrecipitableWater = (row.PrecipitableWater - float(test.PrecipitableWater))**2 * PrecipitableWater_weight\n",
    "    RelativeHumidity = (row.RelativeHumidity - float(test.RelativeHumidity))**2 * RelativeHumidity_weight\n",
    "    SnowWater = (row.SnowWater - float(test.SnowWater))**2 * SnowWater_weight\n",
    "    SurfacePressure = (row.SurfacePressure - float(test.SurfacePressure))**2 * SurfacePressure_weight\n",
    "    WindDirection10m = (row.WindDirection10m - float(test.WindDirection10m))**2 * WindDirection10m_weight\n",
    "    WindSpeed10m = (row.WindSpeed10m - float(test.WindSpeed10m))**2 * WindSpeed10m_weight\n",
    "    Zenith = (row.Zenith - float(test.Zenith))**2 * Zenith_weight\n",
    "    AlbedoDaily = (row.AlbedoDaily - float(test.AlbedoDaily))**2 * AlbedoDaily_weight\n",
    "    hour = (row.hour - float(test.hour))**2 * hour_weight\n",
    "    weekday = (row.weekday - float(test.weekday))**2 * weekday_weight\n",
    "    month = (row.month - float(test.month))**2 * month_weight\n",
    "    year = (row.year - float(test.year))**2 * year_weight\n",
    "    is_weekend = (row.is_weekend - float(test.is_weekend))**2 * is_weekend_weight\n",
    "    is_public_holiday = (0 - 0)**2 * is_public_holiday_weight\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_diff = AirTemp+Azimuth+CloudOpacity+DewpointTemp+Dhi+Dni+Ebh+Ghi+PrecipitableWater+RelativeHumidity+SnowWater+SurfacePressure+WindDirection10m+WindSpeed10m+Zenith+AlbedoDaily+hour+weekday+month+year+is_weekend+is_public_holiday   \n",
    "    if total_diff < min_diff and row.year != 2017:\n",
    "        min_diff = total_diff\n",
    "        print(min_diff)\n",
    "        print(row.date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(test.AirTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9fcd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
